{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9234521",
   "metadata": {},
   "source": [
    "This notebook trains a binary classifier (benign vs. malignant) on BCN20000 and reports Accuracy, Recall, AUROC.\n",
    "We also apply validation-time test-time augmentation (TTA) using horizontal flips and average probabilities to stabilize AUROC.\n",
    "A trained model checkpoint is saved to ../artifacts/checkpoints/ for reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f898f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    "    confusion_matrix,\n",
    "    average_precision_score,\n",
    "    brier_score_loss\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv, json, datetime\n",
    "\n",
    "from src import load_bcn20000, get_transforms\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd655d7",
   "metadata": {},
   "source": [
    "---\n",
    "Helper classes and utilities: device selection, dataset wrapper, and dataloaders with the binary mapping (malignant vs benign)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8fc2461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "class TorchImageDataset(Dataset):\n",
    "    def __init__(self, hf_ds, transform, has_labels=True):\n",
    "        self.ds = hf_ds\n",
    "        self.tf = transform\n",
    "        self.has_labels = has_labels\n",
    "        self.label_feature = hf_ds.features[\"label\"] if has_labels else None\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.ds[idx]\n",
    "        img = ex[\"image\"]\n",
    "        if not isinstance(img, Image.Image):\n",
    "            img = Image.open(img).convert(\"RGB\")\n",
    "        x = self.tf(img)\n",
    "        if self.has_labels:\n",
    "            y = ex[\"label\"]\n",
    "            if isinstance(y, str):\n",
    "                y = self.label_feature.str2int(y)\n",
    "            else:\n",
    "                y = int(y)\n",
    "            return x, y\n",
    "        return x\n",
    "\n",
    "def get_binary_mapping():\n",
    "    malignant = {\"MEL\",\"SCC\",\"BCC\"}\n",
    "    all_labels = [\"MEL\",\"SCC\",\"NV\",\"BCC\",\"BKL\",\"AK\",\"DF\",\"VASC\"]\n",
    "    return {lbl: (\"malignant\" if lbl in malignant else \"benign\") for lbl in all_labels}\n",
    "\n",
    "def make_loaders(batch_size=64):\n",
    "    label_mapping = get_binary_mapping()\n",
    "    train_hf = load_bcn20000(split=\"train\", filename_column=\"bcn_filename\", label_column=\"diagnosis\", label_mapping=label_mapping)\n",
    "    val_hf   = load_bcn20000(split=\"validation\", filename_column=\"bcn_filename\", label_column=\"diagnosis\", label_mapping=label_mapping)\n",
    "    label_names = train_hf.features[\"label\"].names\n",
    "    train_tf = get_transforms(train=True)\n",
    "    eval_tf  = get_transforms(train=False)\n",
    "    train_ds = TorchImageDataset(train_hf, train_tf, has_labels=True)\n",
    "    val_ds   = TorchImageDataset(val_hf,   eval_tf,  has_labels=True)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=0)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    return train_loader, val_loader, label_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c0deab",
   "metadata": {},
   "source": [
    "---\n",
    "Model: ResNet18 with a 2-class head. Training loop runs for a few epochs and returns the model plus its history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52ddada6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_resnet18_binary(num_classes=2, use_pretrained=True):\n",
    "    try:\n",
    "        if use_pretrained:\n",
    "            weights = models.ResNet18_Weights.DEFAULT\n",
    "            model = models.resnet18(weights=weights)\n",
    "        else:\n",
    "            model = models.resnet18(weights=None)\n",
    "    except Exception:\n",
    "        model = models.resnet18(weights=None)\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "def evaluate_basic(model, data_loader, device, criterion):\n",
    "    model.eval()\n",
    "    total, correct, running = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in data_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "            running += loss.item()\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += (pred == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "    return running / max(1, len(data_loader)), correct / max(1, total)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, epochs=15, lr=1e-3):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    history = {\"loss\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "    from tqdm.auto import tqdm\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\", leave=True)\n",
    "        for xb, yb in pbar:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running += loss.item()\n",
    "            pbar.set_postfix(batch_loss=f\"{loss.item():.4f}\")\n",
    "        tr_loss = running / max(1, len(train_loader))\n",
    "        vl_loss, vl_acc = evaluate_basic(model, val_loader, device, criterion)\n",
    "        history[\"loss\"].append(tr_loss)\n",
    "        history[\"val_loss\"].append(vl_loss)\n",
    "        history[\"val_acc\"].append(vl_acc)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c802e3e1",
   "metadata": {},
   "source": [
    "---\n",
    "Validation-time TTA: average probabilities from original and horizontally flipped images to stabilize AUROC and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af2d600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_with_tta(model, xb, device):\n",
    "    model.eval()\n",
    "    xb = xb.to(device)\n",
    "    out = model(xb)\n",
    "    prob = torch.softmax(out, dim=1)\n",
    "    xb_flip = TF.hflip(xb)\n",
    "    out_flip = model(xb_flip)\n",
    "    prob_flip = torch.softmax(out_flip, dim=1)\n",
    "    prob_avg = (prob + prob_flip) / 2.0\n",
    "    return prob_avg\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_outputs(model, loader, device, use_tta=True):\n",
    "    ys, preds, probs = [], [], []\n",
    "    for xb, yb in loader:\n",
    "        if use_tta:\n",
    "            pr = predict_with_tta(model, xb, device).cpu()\n",
    "        else:\n",
    "            out = model(xb.to(device))\n",
    "            pr = torch.softmax(out, dim=1).cpu()\n",
    "        pb = pr[:, 1]\n",
    "        preds.extend(pr.argmax(dim=1).tolist())\n",
    "        probs.extend(pb.tolist())\n",
    "        ys.extend(yb.tolist())\n",
    "    return ys, preds, probs\n",
    "\n",
    "def compute_core_metrics(y_true, y_pred, y_prob, pos_label_idx):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred, pos_label=pos_label_idx, zero_division=0)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "    except Exception:\n",
    "        auc = float(\"nan\")\n",
    "    return {\"accuracy\": float(acc), \"recall\": float(rec), \"auroc\": float(auc)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99441bb4",
   "metadata": {},
   "source": [
    "---\n",
    "Plotting utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90660ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_dir(path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "def plot_loss_acc(history, outdir=\"../artifacts/plots\", tag=\"binary_resnet18\"):\n",
    "    ensure_dir(outdir)\n",
    "    fig = plt.figure(figsize=(7,4))\n",
    "    plt.plot(history[\"loss\"], label=\"train_loss\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Loss: {tag}\")\n",
    "    p1 = os.path.join(outdir, f\"{tag}_loss.png\")\n",
    "    fig.savefig(p1, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    fig = plt.figure(figsize=(7,4))\n",
    "    plt.plot(history[\"val_acc\"], label=\"val_acc\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Val Acc: {tag}\")\n",
    "    p2 = os.path.join(outdir, f\"{tag}_val_acc.png\")\n",
    "    fig.savefig(p2, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    return p1, p2\n",
    "\n",
    "def plot_roc_curve(y_true, y_prob, outdir=\"../artifacts/plots\", tag=\"binary_resnet18\"):\n",
    "    ensure_dir(outdir)\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    plt.plot(fpr, tpr, label=f\"AUROC={auc:.3f}\")\n",
    "    plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend()\n",
    "    path = os.path.join(outdir, f\"{tag}_roc.png\")\n",
    "    fig.savefig(path, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    return path\n",
    "\n",
    "def plot_pr_curve(y_true, y_prob, outdir=\"../artifacts/plots\", tag=\"binary_resnet18\"):\n",
    "    ensure_dir(outdir)\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
    "    ap = average_precision_score(y_true, y_prob)\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    plt.plot(recall, precision, label=f\"AP={ap:.3f}\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision-Recall Curve\")\n",
    "    plt.legend()\n",
    "    path = os.path.join(outdir, f\"{tag}_pr.png\")\n",
    "    fig.savefig(path, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    return path\n",
    "\n",
    "def plot_confusion(y_true, y_pred, label_names, outdir=\"../artifacts/plots\", tag=\"binary_resnet18\"):\n",
    "    ensure_dir(outdir)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "    fig = plt.figure(figsize=(6,5))\n",
    "    plt.imshow(cm, cmap=\"Blues\")\n",
    "    plt.xticks([0,1], label_names)\n",
    "    plt.yticks([0,1], label_names)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j, i, str(cm[i,j]), ha=\"center\", va=\"center\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    path = os.path.join(outdir, f\"{tag}_confusion.png\")\n",
    "    fig.savefig(path, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    return path\n",
    "\n",
    "def plot_calibration(y_true, y_prob, outdir=\"../artifacts/plots\", tag=\"binary_resnet18\", bins=10):\n",
    "    ensure_dir(outdir)\n",
    "    y_true = np.array(y_true)\n",
    "    y_prob = np.array(y_prob)\n",
    "    edges = np.linspace(0,1,bins+1)\n",
    "    idx = np.digitize(y_prob, edges) - 1\n",
    "    acc = []\n",
    "    conf = []\n",
    "    for b in range(bins):\n",
    "        m = idx == b\n",
    "        if m.sum() == 0:\n",
    "            continue\n",
    "        acc.append(y_true[m].mean())\n",
    "        conf.append(y_prob[m].mean())\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "    plt.scatter(conf, acc)\n",
    "    plt.xlabel(\"Predicted probability\")\n",
    "    plt.ylabel(\"Empirical accuracy\")\n",
    "    plt.title(\"Calibration Plot\")\n",
    "    path = os.path.join(outdir, f\"{tag}_calibration.png\")\n",
    "    fig.savefig(path, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    return path\n",
    "\n",
    "def plot_threshold_sweep(y_true, y_prob, label_names, outdir=\"../artifacts/plots\", tag=\"binary_resnet18\"):\n",
    "    ensure_dir(outdir)\n",
    "    y_true = np.array(y_true)\n",
    "    y_prob = np.array(y_prob)\n",
    "    ts = np.linspace(0.0, 1.0, 101)\n",
    "    recalls = []\n",
    "    accuracies = []\n",
    "    for t in ts:\n",
    "        y_pred = (y_prob >= t).astype(int)\n",
    "        recalls.append(recall_score(y_true, y_pred, pos_label=1, zero_division=0))\n",
    "        accuracies.append(accuracy_score(y_true, y_pred))\n",
    "    fig = plt.figure(figsize=(7,4))\n",
    "    plt.plot(ts, recalls, label=f\"Recall({label_names[1]})\")\n",
    "    plt.plot(ts, accuracies, label=\"Accuracy\")\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Threshold Sweep\")\n",
    "    plt.legend()\n",
    "    path = os.path.join(outdir, f\"{tag}_threshold_sweep.png\")\n",
    "    fig.savefig(path, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f3235b",
   "metadata": {},
   "source": [
    "---\n",
    "Save a reusable checkpoint and a JSON of the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9814559c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, path=\"../artifacts/checkpoints/binary_resnet18.pt\"):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    torch.save({\"state_dict\": model.state_dict()}, path)\n",
    "    return path\n",
    "\n",
    "def save_metrics_json(metrics, path=\"../artifacts/binary_metrics.json\"):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    return path\n",
    "\n",
    "def append_metrics_log(metrics, log_path=\"../artifacts/metrics_log.csv\", run_name=\"binary_resnet18\"):\n",
    "    os.makedirs(os.path.dirname(log_path), exist_ok=True)\n",
    "    file_exists = os.path.isfile(log_path)\n",
    "    with open(log_path, \"a\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        if not file_exists:\n",
    "            writer.writerow([\"timestamp\", \"run_name\", \"accuracy\", \"recall\", \"auroc\", \"epochs\", \"batch_size\", \"lr\"])\n",
    "        writer.writerow([\n",
    "            datetime.datetime.now().isoformat(timespec=\"seconds\"),\n",
    "            run_name,\n",
    "            metrics.get(\"accuracy\", \"\"),\n",
    "            metrics.get(\"recall\", \"\"),\n",
    "            metrics.get(\"auroc\", \"\"),\n",
    "            metrics.get(\"epochs\", \"\"),\n",
    "            metrics.get(\"batch_size\", \"\"),\n",
    "            metrics.get(\"lr\", \"\")\n",
    "        ])\n",
    "    return log_path\n",
    "\n",
    "def save_history_csv(history, path=\"../artifacts/history_binary_resnet18.csv\"):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"epoch\", \"train_loss\", \"val_loss\", \"val_acc\"])\n",
    "        for i, (tr, vl, va) in enumerate(zip(history[\"loss\"], history[\"val_loss\"], history[\"val_acc\"]), start=1):\n",
    "            writer.writerow([i, tr, vl, va])\n",
    "    return path\n",
    "\n",
    "def save_predictions_csv(y_true, y_pred, y_prob, label_names, path=\"../artifacts/val_predictions_binary.csv\"):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"y_true\", \"y_pred\", \"p_malignant\"])\n",
    "        for t, p, s in zip(y_true, y_pred, y_prob):\n",
    "            writer.writerow([label_names[t], label_names[p], s])\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620bb1cf",
   "metadata": {},
   "source": [
    "---\n",
    "Train, setup and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15d1d080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(epochs=15, batch_size=64, lr=1e-3, run_name=\"binary_resnet18\"):\n",
    "    device = get_device()\n",
    "    train_loader, val_loader, label_names = make_loaders(batch_size=batch_size)\n",
    "    model = build_resnet18_binary(num_classes=2, use_pretrained=True).to(device)\n",
    "    model, history = train_model(model, train_loader, val_loader, device, epochs=epochs, lr=lr)\n",
    "    y_true, y_pred, y_prob = collect_outputs(model, val_loader, device, use_tta=True)\n",
    "    metrics = compute_core_metrics(y_true, y_pred, y_prob, pos_label_idx=label_names.index(\"malignant\"))\n",
    "    metrics[\"epochs\"] = epochs\n",
    "    metrics[\"batch_size\"] = batch_size\n",
    "    metrics[\"lr\"] = lr\n",
    "    ckpt = save_checkpoint(model, \"../artifacts/checkpoints/binary_resnet18.pt\")\n",
    "    mjson = save_metrics_json(metrics, \"../artifacts/binary_metrics.json\")\n",
    "    mlog = append_metrics_log(metrics, \"../artifacts/metrics_log.csv\", run_name=run_name)\n",
    "    hcsv = save_history_csv(history, \"../artifacts/history_binary_resnet18.csv\")\n",
    "    pcsv = save_predictions_csv(y_true, y_pred, y_prob, label_names, \"../artifacts/val_predictions_binary.csv\")\n",
    "    plots_dir = \"../artifacts/plots\"\n",
    "    p1, p2 = plot_loss_acc(history, plots_dir, run_name)\n",
    "    prc = plot_pr_curve(y_true, y_prob, plots_dir, run_name)\n",
    "    roc = plot_roc_curve(y_true, y_prob, plots_dir, run_name)\n",
    "    cmx = plot_confusion(y_true, y_pred, label_names, plots_dir, run_name)\n",
    "    cal = plot_calibration(y_true, y_prob, plots_dir, run_name)\n",
    "    thr = plot_threshold_sweep(y_true, y_prob, label_names, plots_dir, run_name)\n",
    "    return {\"metrics\": metrics, \"checkpoint\": ckpt, \"metrics_json\": mjson, \"metrics_log\": mlog, \"history_csv\": hcsv, \"predictions_csv\": pcsv, \"plots\": [p1,p2,prc,roc,cmx,cal,thr]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3262c32a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92b29d8fa5d148689080915c51e69d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/12413 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e466b1206445f1a1777acdc8e3e9f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/12413 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9238953d60ba4763838ea7605dbbdd8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/15:   0%|          | 0/194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb87e200c0184b318f680fcdca68f4ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/15:   0%|          | 0/194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca60e26db1384bb296c3e138a04ab173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/15:   0%|          | 0/194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f556c8f005114d25bb0929674189e228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/15:   0%|          | 0/194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b82e861512b4dccb10ecabb4b04a388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/15:   0%|          | 0/194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245c9f18cf2247cd88af7098c5c24285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/15:   0%|          | 0/194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76eee863894f418aac29e7bffe2d32e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/15:   0%|          | 0/194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a72bde4e0c7948b4a5bd99a2ad624f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/15:   0%|          | 0/194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "188e20b919c74722a2c73ee670a2a167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/15:   0%|          | 0/194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7045c507fc048319c9a5e3c132a4d6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/15:   0%|          | 0/194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd5e2a8e24fd42a6889ce9cbead726cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/15:   0%|          | 0/194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66eff8ff4dce4279aa62b32ff640833c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/15:   0%|          | 0/194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d767ab0c234a4f2cb172233c7e421544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/15:   0%|          | 0/194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2995b2fdbba745719311506ce4bfd673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/15:   0%|          | 0/194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25fc7b3429144557a5f081e1938b9292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/15:   0%|          | 0/194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'metrics': {'accuracy': 0.8661290322580645,\n",
       "  'recall': 0.9540229885057471,\n",
       "  'auroc': 0.9561776729927994,\n",
       "  'epochs': 15,\n",
       "  'batch_size': 64,\n",
       "  'lr': 0.001},\n",
       " 'checkpoint': '../artifacts/checkpoints/binary_resnet18.pt',\n",
       " 'metrics_json': '../artifacts/binary_metrics.json',\n",
       " 'metrics_log': '../artifacts/metrics_log.csv',\n",
       " 'history_csv': '../artifacts/history_binary_resnet18.csv',\n",
       " 'predictions_csv': '../artifacts/val_predictions_binary.csv',\n",
       " 'plots': ['../artifacts/plots/binary_resnet18_tta15e_loss.png',\n",
       "  '../artifacts/plots/binary_resnet18_tta15e_val_acc.png',\n",
       "  '../artifacts/plots/binary_resnet18_tta15e_pr.png',\n",
       "  '../artifacts/plots/binary_resnet18_tta15e_roc.png',\n",
       "  '../artifacts/plots/binary_resnet18_tta15e_confusion.png',\n",
       "  '../artifacts/plots/binary_resnet18_tta15e_calibration.png',\n",
       "  '../artifacts/plots/binary_resnet18_tta15e_threshold_sweep.png']}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = run_all(epochs=15, batch_size=64, lr=1e-3, run_name=\"binary_resnet18_tta15e\")\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
